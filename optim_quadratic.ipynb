{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0416cf2",
   "metadata": {},
   "source": [
    "# Optimisation quadratique — gradient & CG (ultra complet)\n",
    "\n",
    "Problème : $f(x)=\\tfrac12 x^\\top A x - b^\\top x$, avec $A=\\text{tridiag}(-1,2,-1)$ (SPD) et $b=\\mathbf{1}$.\n",
    "\n",
    "Ce notebook contient :\n",
    "- Visualisations 2D (surface, contours, champ de gradient, trajectoires des itérés) ;\n",
    "- Méthodes : **gradient pas constant**, **gradient avec recherche exacte**, **backtracking (Armijo)**, **Barzilai–Borwein (BB1)**, **Conjugate Gradient** ;\n",
    "- Courbes de convergence et tableau récapitulatif ;\n",
    "- Cellules prêtes à l’emploi pour $n$ grand (par défaut $n=100$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b498dd79",
   "metadata": {},
   "source": [
    "## 0) Imports & utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a7a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from numpy.linalg import norm, eigvalsh, solve\n",
    "from dataclasses import dataclass\n",
    "\n",
    "def make_tridiag_A(n: int) -> np.ndarray:\n",
    "    d = 2.0 * np.ones(n); o = -1.0 * np.ones(n-1)\n",
    "    return np.diag(d) + np.diag(o,1) + np.diag(o,-1)\n",
    "\n",
    "def make_b(n: int) -> np.ndarray:\n",
    "    return np.ones(n)\n",
    "\n",
    "def f_quad(A,b,x): return 0.5 * x @ (A @ x) - b @ x\n",
    "def grad_f(A,b,x): return A @ x - b\n",
    "\n",
    "@dataclass\n",
    "class Trace:\n",
    "    xs: list\n",
    "    fvals: list\n",
    "    gradnorms: list\n",
    "    times: list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7cde45",
   "metadata": {},
   "source": [
    "## 1) Algorithmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0fa069",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "def run_gradient_constant(A,b,x0,alpha=1e-2,tol=1e-8,maxit=10_000):\n",
    "    x = x0.copy()\n",
    "    xs,fvals,gn,times=[x.copy()],[f_quad(A,b,x)],[norm(grad_f(A,b,x))],[0.0]\n",
    "    t0=time.time()\n",
    "    for _ in range(maxit):\n",
    "        g = grad_f(A,b,x)\n",
    "        if norm(g)<=tol: break\n",
    "        x = x - alpha*g\n",
    "        xs.append(x.copy()); fvals.append(f_quad(A,b,x)); gn.append(norm(grad_f(A,b,x))); times.append(time.time()-t0)\n",
    "    return Trace(xs,fvals,gn,times)\n",
    "\n",
    "def run_gradient_exact(A,b,x0,tol=1e-8,maxit=10_000):\n",
    "    x = x0.copy()\n",
    "    xs,fvals,gn,times=[x.copy()],[f_quad(A,b,x)],[norm(grad_f(A,b,x))],[0.0]\n",
    "    t0=time.time()\n",
    "    for _ in range(maxit):\n",
    "        g = grad_f(A,b,x)\n",
    "        if norm(g)<=tol: break\n",
    "        alpha = (g@g)/(g@(A@g))\n",
    "        x = x - alpha*g\n",
    "        xs.append(x.copy()); fvals.append(f_quad(A,b,x)); gn.append(norm(grad_f(A,b,x))); times.append(time.time()-t0)\n",
    "    return Trace(xs,fvals,gn,times)\n",
    "\n",
    "def run_backtracking(A,b,x0,alpha0=1.0,beta=0.5,c=1e-4,tol=1e-8,maxit=10_000):\n",
    "    x = x0.copy()\n",
    "    xs,fvals,gn,times=[x.copy()],[f_quad(A,b,x)],[norm(grad_f(A,b,x))],[0.0]\n",
    "    t0=time.time()\n",
    "    for _ in range(maxit):\n",
    "        g = grad_f(A,b,x)\n",
    "        if norm(g)<=tol: break\n",
    "        f_x = f_quad(A,b,x); d = -g; alpha = alpha0\n",
    "        while f_quad(A,b,x+alpha*d) > f_x + c*alpha*(g@d):\n",
    "            alpha *= beta\n",
    "        x = x + alpha*d\n",
    "        xs.append(x.copy()); fvals.append(f_quad(A,b,x)); gn.append(norm(grad_f(A,b,x))); times.append(time.time()-t0)\n",
    "    return Trace(xs,fvals,gn,times)\n",
    "\n",
    "def run_barzilai_borwein(A,b,x0,tol=1e-8,maxit=10_000):\n",
    "    x = x0.copy(); g = grad_f(A,b,x)\n",
    "    alpha = 1.0/max(1e-12, eigvalsh(A).max())\n",
    "    xs,fvals,gn,times=[x.copy()],[f_quad(A,b,x)],[norm(g)],[0.0]\n",
    "    t0=time.time()\n",
    "    for _ in range(maxit):\n",
    "        if norm(g)<=tol: break\n",
    "        x_new = x - alpha*g; g_new = grad_f(A,b,x_new)\n",
    "        s = x_new - x; y = g_new - g; denom = s@y\n",
    "        if denom>1e-20: alpha = (s@s)/denom\n",
    "        x,g = x_new,g_new\n",
    "        xs.append(x.copy()); fvals.append(f_quad(A,b,x)); gn.append(norm(g)); times.append(time.time()-t0)\n",
    "    return Trace(xs,fvals,gn,times)\n",
    "\n",
    "def run_conjugate_gradient(A,b,x0,tol=1e-8,maxit=None):\n",
    "    n=len(x0); \n",
    "    if maxit is None: maxit=n\n",
    "    x = x0.copy(); r = b - A@x; p = r.copy()\n",
    "    xs,fvals,gn,times=[x.copy()],[f_quad(A,b,x)],[norm(-r)],[0.0]\n",
    "    t0=time.time()\n",
    "    for _ in range(maxit):\n",
    "        if norm(r)<=tol: break\n",
    "        Ap = A@p; alpha = (r@r)/(p@Ap)\n",
    "        x = x + alpha*p; r_new = r - alpha*Ap; beta = (r_new@r_new)/(r@r); p = r_new + beta*p; r = r_new\n",
    "        xs.append(x.copy()); fvals.append(f_quad(A,b,x)); gn.append(norm(-r)); times.append(time.time()-t0)\n",
    "    return Trace(xs,fvals,gn,times)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2710336",
   "metadata": {},
   "source": [
    "## 2) Visualisation complète pour $n=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177c358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A = make_tridiag_A(2); b = make_b(2); x_star = solve(A,b)\n",
    "x0 = np.array([5.0,10.0])\n",
    "\n",
    "runs = {\n",
    "    \"Grad cst (α=0.01)\": run_gradient_constant(A,b,x0,alpha=1e-2,tol=1e-12,maxit=5000),\n",
    "    \"Grad exact\": run_gradient_exact(A,b,x0,tol=1e-12,maxit=5000),\n",
    "    \"Backtracking\": run_backtracking(A,b,x0,tol=1e-12,maxit=5000),\n",
    "    \"BB1\": run_barzilai_borwein(A,b,x0,tol=1e-12,maxit=5000),\n",
    "    \"CG\": run_conjugate_gradient(A,b,x0,tol=1e-12,maxit=100)\n",
    "}\n",
    "\n",
    "# Contours + trajectoires + champ -∇f\n",
    "xs = np.linspace(-10,10,200); ys = np.linspace(-10,10,200); X,Y = np.meshgrid(xs,ys)\n",
    "Z = 0.5*(A[0,0]*X**2 + 2*A[0,1]*X*Y + A[1,1]*Y**2) - (b[0]*X + b[1]*Y)\n",
    "plt.figure(); plt.contour(X,Y,Z,levels=30)\n",
    "U = -(A[0,0]*X + A[0,1]*Y - b[0]); V = -(A[1,0]*X + A[1,1]*Y - b[1])\n",
    "plt.quiver(X[::12,::12],Y[::12,::12],U[::12,::12],V[::12,::12])\n",
    "for name,tr in runs.items():\n",
    "    pts = np.vstack(tr.xs); plt.plot(pts[:,0],pts[:,1],'-o',label=name)\n",
    "plt.xlabel('x1'); plt.ylabel('x2'); plt.title('Trajectoires (n=2)'); plt.legend(); plt.show()\n",
    "\n",
    "# Convergence\n",
    "f_star = f_quad(A,b,x_star)\n",
    "plt.figure()\n",
    "for name,tr in runs.items():\n",
    "    plt.semilogy(np.maximum(1e-20, np.array(tr.fvals)-f_star), label=name)\n",
    "plt.xlabel('itérations'); plt.ylabel('f(x)-f*'); plt.title('Convergence f'); plt.legend(); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for name,tr in runs.items():\n",
    "    plt.semilogy(np.maximum(1e-20, np.array(tr.gradnorms)), label=name)\n",
    "plt.xlabel('itérations'); plt.ylabel('||∇f||'); plt.title('Convergence ||∇f||'); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0313de5c",
   "metadata": {},
   "source": [
    "## 3) Expériences en grande dimension (par défaut $n=100$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5ee566",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 100\n",
    "A = make_tridiag_A(n); b = make_b(n); x_star = solve(A,b)\n",
    "evals = eigvalsh(A); L,mu = evals.max(), evals.min(); kappa = L/mu\n",
    "x0 = np.zeros(n)\n",
    "\n",
    "# Paramètres plus légers pour éviter des temps longs si vous exécutez en ligne\n",
    "runs = {\n",
    "    \"Grad cst (α=1/L)\": run_gradient_constant(A,b,x0,alpha=1.0/L,tol=1e-10,maxit=5000),\n",
    "    \"Grad exact\": run_gradient_exact(A,b,x0,tol=1e-10,maxit=2000),\n",
    "    \"Backtracking\": run_backtracking(A,b,x0,alpha0=1.0,beta=0.5,c=1e-4,tol=1e-10,maxit=5000),\n",
    "    \"BB1\": run_barzilai_borwein(A,b,x0,tol=1e-10,maxit=5000),\n",
    "    \"CG\": run_conjugate_gradient(A,b,x0,tol=1e-10,maxit=500)\n",
    "}\n",
    "\n",
    "f_star = f_quad(A,b,x_star)\n",
    "plt.figure()\n",
    "for name,tr in runs.items():\n",
    "    plt.semilogy(np.maximum(1e-30,np.array(tr.fvals)-f_star),label=name)\n",
    "plt.xlabel('itérations'); plt.ylabel('f(x)-f*'); plt.title(f'Convergence f (n={n}, κ={kappa:.1f})'); plt.legend(); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for name,tr in runs.items():\n",
    "    plt.semilogy(np.maximum(1e-30,np.array(tr.gradnorms)),label=name)\n",
    "plt.xlabel('itérations'); plt.ylabel('||∇f||'); plt.title(f'Convergence gradient (n={n})'); plt.legend(); plt.show()\n",
    "\n",
    "# Tableau récapitulatif\n",
    "rows = []\n",
    "for name,tr in runs.items():\n",
    "    rows.append({\"méthode\": name, \"itérations\": len(tr.fvals)-1,\n",
    "                 \"f(x_end)-f*\": tr.fvals[-1]-f_star, \"||∇f||_end\": tr.gradnorms[-1]})\n",
    "import pandas as pd\n",
    "pd.DataFrame(rows).sort_values(\"itérations\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
